//
//  CameraManager.swift
//  PisionTest1
//
//  Created by ì—¬ì„±ì¼ on 7/9/25.
//

import AVFoundation
import SwiftUI
import Vision
import CoreML

final class CameraManager: NSObject, ObservableObject {
  let session = AVCaptureSession()
  @Published var currentState = "ëŒ€ê¸° ì¤‘..."
  
  // ê´€ì ˆ ì¢Œí‘œë¥¼ ì €ì¥í•  í”„ë¡œí¼í‹° ì¶”ê°€
  @Published var bodyPosePoints: [VNHumanBodyPoseObservation.JointName: VNRecognizedPoint] = [:]
  
  private let videoOutput = AVCaptureVideoDataOutput()
  private var isSessionConfigured = false
  private let sessionQueue = DispatchQueue(label: "CameraSessionQueue")
  private let processingQueue = DispatchQueue(label: "ProcessingQueue")
  
  // Vision ê´€ë ¨ í”„ë¡œí¼í‹°
  private var bodyPoseRequest: VNDetectHumanBodyPoseRequest?
  private var mlModel: PisionTestModel?
  
  override init() {
    super.init()
    setupVision()
  }
  
  private func setupVision() {
    // Core ML ëª¨ë¸ ë¡œë“œ
    do {
      let config = MLModelConfiguration()
      mlModel = try PisionTestModel(configuration: config)
      print("PisionTestModel ë¡œë“œ ì„±ê³µ")
    } catch {
      print("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: \(error)")
    }
    
    // Body Pose ê°ì§€ ìš”ì²­ ì„¤ì •
    bodyPoseRequest = VNDetectHumanBodyPoseRequest { [weak self] request, error in
      if let error = error {
        print("Body pose ê°ì§€ ì—ëŸ¬: \(error)")
        return
      }
      
      self?.processBodyPoseObservations(request.results as? [VNHumanBodyPoseObservation])
    }
  }
  
  func startSession() {
    sessionQueue.async {
      if !self.session.isRunning {
        self.session.startRunning()
      }
    }
  }
  
  func stopSession() {
    sessionQueue.async {
      if self.session.isRunning {
        self.session.stopRunning()
      }
    }
  }
  
  func requestAndCheckPermissions() {
    switch AVCaptureDevice.authorizationStatus(for: .video) {
    case .notDetermined:
      AVCaptureDevice.requestAccess(for: .video) { [weak self] granted in
        if granted {
          self?.configureSessionIfNeeded()
        } else {
          print("ì‚¬ìš©ìê°€ ì¹´ë©”ë¼ ì ‘ê·¼ì„ ê±°ë¶€í–ˆìŠµë‹ˆë‹¤.")
        }
      }
      
    case .authorized:
      configureSessionIfNeeded()
      
    case .restricted, .denied:
      print("ì¹´ë©”ë¼ ì ‘ê·¼ì´ ì œí•œë˜ì—ˆê±°ë‚˜ ê±°ë¶€ë¨")
      
    @unknown default:
      print("ì•Œ ìˆ˜ ì—†ëŠ” ê¶Œí•œ ìƒíƒœ")
    }
  }
}

// MARK: - AVCaptureVideoDataOutputSampleBufferDelegate
extension CameraManager: AVCaptureVideoDataOutputSampleBufferDelegate {
  func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
    guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
    
    processingQueue.async { [weak self] in
      self?.detectBodyPose(in: pixelBuffer)
    }
  }
}

// MARK: - Private Methods
extension CameraManager {
  private func configureSession() {
    session.beginConfiguration()
    
    // ì¹´ë©”ë¼ ì…ë ¥ ì„¤ì •
    guard let device = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .front),
          let input = try? AVCaptureDeviceInput(device: device),
          session.canAddInput(input) else {
      print("Log: ì¹´ë©”ë¼ ì¸í’‹ ì„¤ì • ì‹¤íŒ¨")
      session.commitConfiguration()
      return
    }
    
    session.addInput(input)
    
    // ë¹„ë””ì˜¤ ì¶œë ¥ ì„¤ì •
    videoOutput.setSampleBufferDelegate(self, queue: processingQueue)
    videoOutput.alwaysDiscardsLateVideoFrames = true
    videoOutput.videoSettings = [
      kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_420YpCbCr8BiPlanarFullRange
    ]
    
    if session.canAddOutput(videoOutput) {
      session.addOutput(videoOutput)
      
      // ë¹„ë””ì˜¤ ë°©í–¥ ì„¤ì •
      if let connection = videoOutput.connection(with: .video) {
        if #available(iOS 17.0, *) {
          connection.videoRotationAngle = 0 // portrait
        } else {
          connection.videoOrientation = .portrait
        }
        if connection.isVideoMirroringSupported {
          connection.isVideoMirrored = true
        }
      }
    }
    
    session.commitConfiguration()
  }
  
  private func configureSessionIfNeeded() {
    guard !isSessionConfigured else {
      print("ì´ë¯¸ ì„¸ì…˜ì´ êµ¬ì„±ë˜ì–´ ìˆìŒ")
      return
    }
    isSessionConfigured = true
    configureSession()
  }
  
  private func detectBodyPose(in pixelBuffer: CVPixelBuffer) {
    guard let request = bodyPoseRequest else { return }
    
    let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, orientation: .up, options: [:])
    
    do {
      try handler.perform([request])
    } catch {
      print("Body pose ê°ì§€ ì‹¤í–‰ ì‹¤íŒ¨: \(error)")
    }
  }
  
  private func processBodyPoseObservations(_ observations: [VNHumanBodyPoseObservation]?) {
    guard let observations = observations,
          let observation = observations.first else {
      // ì‚¬ëŒì´ ê°ì§€ë˜ì§€ ì•ŠìŒ
      DispatchQueue.main.async { [weak self] in
        self?.currentState = "ì‚¬ëŒì´ ê°ì§€ë˜ì§€ ì•ŠìŒ"
        self?.bodyPosePoints = [:]
      }
      return
    }
    
    // ëª¨ë“  ê´€ì ˆ ì¢Œí‘œ ì¶”ì¶œ
    var detectedPoints: [VNHumanBodyPoseObservation.JointName: VNRecognizedPoint] = [:]
    
    do {
      // ì£¼ìš” ê´€ì ˆë“¤
      let joints: [VNHumanBodyPoseObservation.JointName] = [
        .nose, .leftEye, .rightEye, .leftEar, .rightEar,
        .leftShoulder, .rightShoulder, .leftElbow, .rightElbow,
        .leftWrist, .rightWrist, .leftHip, .rightHip,
        .leftKnee, .rightKnee, .leftAnkle, .rightAnkle
      ]
      
      for joint in joints {
        let point = try observation.recognizedPoint(joint)
        if point.confidence > 0.3 { // ì‹ ë¢°ë„ê°€ 0.3 ì´ìƒì¸ ê²ƒë§Œ
          detectedPoints[joint] = point
        }
      }
      
      DispatchQueue.main.async { [weak self] in
        self?.bodyPosePoints = detectedPoints
      }
      
    } catch {
      print("ê´€ì ˆ ì¶”ì¶œ ì‹¤íŒ¨: \(error)")
    }
    
    // Core ML ëª¨ë¸ë¡œ í¬ì¦ˆ ë¶„ë¥˜
    classifyPose(from: observation)
  }
  
  // ì„ì‹œ ì‹œë®¬ë ˆì´ì…˜ í•¨ìˆ˜
  private func simulatePoseClassification(from observation: VNHumanBodyPoseObservation) {
    do {
      // ë¨¸ë¦¬ì™€ ì–´ê¹¨ì˜ ìƒëŒ€ì  ìœ„ì¹˜ë¡œ ê°„ë‹¨í•œ íŒë³„
      let nose = try observation.recognizedPoint(.nose)
      let leftShoulder = try observation.recognizedPoint(.leftShoulder)
      let rightShoulder = try observation.recognizedPoint(.rightShoulder)
      
      if nose.confidence > 0.3 && leftShoulder.confidence > 0.3 && rightShoulder.confidence > 0.3 {
        // ë¨¸ë¦¬ê°€ ì–´ê¹¨ë³´ë‹¤ ë‚®ìœ¼ë©´ ì¡¸ê³  ìˆë‹¤ê³  ê°€ì •
        let shoulderY = (leftShoulder.y + rightShoulder.y) / 2
        let headTilt = nose.y - shoulderY
        
        let state = headTilt > 0.1 ? "Snooze" : "Concentration"
        let confidence = abs(headTilt) * 100
        
        DispatchQueue.main.async { [weak self] in
          self?.currentState = "\(state) (ì‹ ë¢°ë„: \(String(format: "%.1f%%", min(confidence, 95))))"
        }
      }
    } catch {
      print("í¬ì¦ˆ í¬ì¸íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: \(error)")
    }
  }
  
  private func classifyPose(from observation: VNHumanBodyPoseObservation) {
    print("ğŸ” classifyPose ì‹œì‘")
    
    guard let mlModel = mlModel else {
      print("âŒ ML ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
      // ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì‹œë®¬ë ˆì´ì…˜ ì‚¬ìš©
      simulatePoseClassification(from: observation)
      return
    }
    
    print("âœ… ML ëª¨ë¸ ë¡œë“œ í™•ì¸")
    
    do {
      // MLMultiArray ìƒì„± (ëª¨ë¸ì˜ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ)
      // ì…ë ¥ í˜•íƒœ: [120, 3, 18] - 120ê°œ í”„ë ˆì„, 3ê°œ ì¢Œí‘œ(x,y,confidence), 18ê°œ ê´€ì ˆ
      print("ğŸ“Š MLMultiArray ìƒì„± ì‹œë„...")
      let multiArray = try MLMultiArray(shape: [90, 3, 18], dataType: .float32)
      print("âœ… MLMultiArray ìƒì„± ì„±ê³µ")
      
      // ëª¨ë“  ê°’ì„ 0ìœ¼ë¡œ ì´ˆê¸°í™”
      for i in 0..<multiArray.count {
        multiArray[i] = 0
      }
      
      // ê´€ì ˆ ì¸ë±ìŠ¤ ë§¤í•‘ (18ê°œ ê´€ì ˆ)
      let jointMapping: [(VNHumanBodyPoseObservation.JointName, Int)] = [
        (.nose, 0), (.leftEye, 1), (.rightEye, 2), (.leftEar, 3), (.rightEar, 4),
        (.leftShoulder, 5), (.rightShoulder, 6), (.leftElbow, 7), (.rightElbow, 8),
        (.leftWrist, 9), (.rightWrist, 10), (.leftHip, 11), (.rightHip, 12),
        (.leftKnee, 13), (.rightKnee, 14), (.leftAnkle, 15), (.rightAnkle, 16),
        (.neck, 17)  // 18ë²ˆì§¸ ê´€ì ˆ ì¶”ê°€
      ]
      
      // í˜„ì¬ í”„ë ˆì„ì˜ ê´€ì ˆ ë°ì´í„°ë¥¼ ì²« ë²ˆì§¸ í”„ë ˆì„(ì¸ë±ìŠ¤ 0)ì—ë§Œ ì…ë ¥
      print("ğŸ¦´ ê´€ì ˆ ë°ì´í„° ì…ë ¥ ì‹œì‘...")
      var detectedJointCount = 0
      
      for (joint, index) in jointMapping {
        if index < 17 {  // neckì€ Visionì—ì„œ ì§ì ‘ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì²˜ë¦¬
          if let point = try? observation.recognizedPoint(joint) {
            // [í”„ë ˆì„=0, ì¢Œí‘œ, ê´€ì ˆ] ìˆœì„œë¡œ ë°ì´í„° ì…ë ¥
            multiArray[[0, 0, index] as [NSNumber]] = NSNumber(value: Float(point.x))      // x
            multiArray[[0, 1, index] as [NSNumber]] = NSNumber(value: Float(point.y))      // y
            multiArray[[0, 2, index] as [NSNumber]] = NSNumber(value: Float(point.confidence)) // confidence
            detectedJointCount += 1
          }
        } else {
          // neck ê´€ì ˆì€ leftShoulderì™€ rightShoulderì˜ ì¤‘ê°„ì ìœ¼ë¡œ ê³„ì‚°
          if let leftShoulder = try? observation.recognizedPoint(.leftShoulder),
             let rightShoulder = try? observation.recognizedPoint(.rightShoulder) {
            let neckX = (leftShoulder.x + rightShoulder.x) / 2
            let neckY = (leftShoulder.y + rightShoulder.y) / 2
            let neckConfidence = (leftShoulder.confidence + rightShoulder.confidence) / 2
            
            multiArray[[0, 0, 17] as [NSNumber]] = NSNumber(value: Float(neckX))
            multiArray[[0, 1, 17] as [NSNumber]] = NSNumber(value: Float(neckY))
            multiArray[[0, 2, 17] as [NSNumber]] = NSNumber(value: Float(neckConfidence))
            detectedJointCount += 1
          }
        }
      }
      
      print("âœ… ê´€ì ˆ ë°ì´í„° ì…ë ¥ ì™„ë£Œ (ê°ì§€ëœ ê´€ì ˆ: \(detectedJointCount)/18)")
      print("ğŸ“ ì…ë ¥ ë°°ì—´ shape: \(multiArray.shape)")
      
      // ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰
      print("ğŸ¤– ëª¨ë¸ ì˜ˆì¸¡ ì‹œì‘...")
      let input = PisionTestModelInput(poses: multiArray)
      print("âœ… PisionTestModelInput ìƒì„± ì„±ê³µ")
      
      let prediction = try mlModel.model.prediction(from: input)
      print("âœ… ëª¨ë¸ ì˜ˆì¸¡ ì„±ê³µ")
      
      // ê²°ê³¼ ì²˜ë¦¬
      print("ğŸ“ ì˜ˆì¸¡ ê²°ê³¼ ì²˜ë¦¬ ì¤‘...")
      if let output = prediction.featureValue(for: "label")?.stringValue {
        print("âœ… ì˜ˆì¸¡ ë ˆì´ë¸”: \(output)")
          
          DispatchQueue.main.async { [weak self] in
            self?.currentState = output
          }
//        if let confidenceArray = prediction.featureValue(for: "classLabelProbs")?.dictionaryValue {
//          print("âœ… ì‹ ë¢°ë„ ë”•ì…”ë„ˆë¦¬ íšë“")
//          
//          // ì‹ ë¢°ë„ ê°’ ê°€ì ¸ì˜¤ê¸°
//          let confidence = confidenceArray[output] as? Double ?? 0.0
//          print("âœ… ìµœì¢… ì‹ ë¢°ë„: \(confidence)")
//          
//          DispatchQueue.main.async { [weak self] in
//            self?.currentState = "\(output) (ì‹ ë¢°ë„: \(String(format: "%.1f%%", confidence * 100)))"
//          }
//        } else {
//          print("âš ï¸ classLabelProbsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
//        }
      } else {
        print("âš ï¸ labelì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        print("ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ feature ì´ë¦„ë“¤:")
        for featureName in prediction.featureNames {
          print("  - \(featureName)")
        }
      }
      
    } catch {
      print("âŒ í¬ì¦ˆ ë¶„ë¥˜ ì‹¤íŒ¨")
      print("âŒ ì—ëŸ¬ íƒ€ì…: \(type(of: error))")
      print("âŒ ì—ëŸ¬ ìƒì„¸: \(error)")
      print("âŒ ì—ëŸ¬ ë¡œì»¬ë¼ì´ì¦ˆë“œ: \(error.localizedDescription)")
      
      // ì—ëŸ¬ ë°œìƒì‹œ ì„ì‹œ ì‹œë®¬ë ˆì´ì…˜ ì‚¬ìš©
      simulatePoseClassification(from: observation)
    }
  }
}
